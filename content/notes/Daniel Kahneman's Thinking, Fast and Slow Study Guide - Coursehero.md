---
title: "Daniel Kahneman's Thinking, Fast and Slow Study Guide - Coursehero"
tags:
- internetContent
- bookSummary
type: article
fc-calendar: Gregorian Calendar
fc-date: 
year: 2023
month: June
day: 26
creation date: 2023-06-26 18:51
modification date: Monday 26th June 2023 18:51:14
---

#internetContent  #bookSummary 
## Article link:
https://www.coursehero.com/lit/Thinking-Fast-and-Slow/
_____
## Overview

### Author

Daniel Kahneman

### Year Published

2011

### Type

Nonfiction

### Genre

Economics, Nonfiction

### At a Glance

First published in 2011, _Thinking, Fast and Slow_ surveys decades of developments in cognitive psychology and behavioral economics. Author Daniel Kahneman, born on March 5, 1934, in Tel Aviv and raised in France, is a professor of psychology and public affairs emeritus who won the Nobel Prize in Economics in 2002. He is best known as one of the pioneers of the "heuristics and biases" approach to psychology. Kahneman conducts much of his work with his longtime friend and colleague, cognitive psychologist Amos Tversky (1937–96). The work emphasizes the extent to which people rely on intuitive approaches to problem solving—_heuristics_—that speed the process of finding a solution when making decisions. These patterns of behavior lead to predictable _biases_ in thinking with far-reaching implications for business, policy, and day-to-day life. _Thinking, Fast and Slow_ offers an accessible discussion of the heuristics on which people rely and of the illusions and mistaken beliefs to which the human mind most readily falls prey. One of the most appealing and accessible features of _Thinking, Fast and Slow_ is its use of startling and even amusing experimental results to illustrate points. The ideas Kahneman presents are often difficult to follow in their abstract form, but they are easy to recognize when applied to concrete scenarios.

### About the Title

Kahneman begins this survey of heuristics-and-biases psychology, _Thinking, Fast and Slow_, by introducing two "systems" of cognition: one fast, the other slow. Throughout the book, Kahneman shows how the two systems collaborate and describes many circumstances in which fast, intuitive thinking reliably fails.
___
# Main Ideas

Share

## Systems 1 and 2 Govern Human Judgments

For roughly the first half of the book, Kahneman develops the idea of two "systems" of cognitive behavior that together govern human judgments and decisions. System 1 specializes in "thinking fast" and handles essentially every intuitive, spur-of-the-moment judgment a person is asked to make. Examples of System 1 tasks include simple mental arithmetic, facial recognition, and intuitive estimates of an event's likelihood or rarity. Kahneman provides a much more extensive list. System 1 is "always-on" and will often start working to solve a problem before an individual is even consciously aware of the problem. It works _associatively_, conducting a broad but haphazard search for relevant information. Perhaps most importantly for Kahneman's purposes, System 1 operates by means of _heuristics_—fast, approximate rules—that leave it prone to systematic biases in the conclusions it reaches.

In contrast, System 2 represents the "thinking slow" half of the book's title. Kahneman describes this as a "lazy" system that typically confirms the intuitions of System 1 without fully investigating them. Despite its reluctance to get involved in solving a problem, System 2 alone is capable of performing certain kinds of cognitive work. For example, System 2 would conduct the methodical step-by-step reasoning required to interpret statistics or measure the probabilities of highly contingent events. Additionally, System 2 is uniquely equipped to recognize gaps in information and to seek out new information that will fill those gaps. This contrasts sharply with the System 1 tendency to use whatever information is readily at hand, fitting it into a plausible but generally incomplete framework. Anything that commands a person's full and conscious attention is engaging System 2.

The two systems, as Kahneman occasionally pauses to emphasize, are not distinct regions of the brain or separate neural pathways. Instead, they can be thought of as systematic patterns of thought, each with their own physiological and behavioral markers. The intense concentration characteristic of System 2 thinking, for instance, can be measured in the form of an elevated heart rate and dilated pupils, phenomena that diminish when a person ceases to exert focused cognitive effort. The value of this "two systems" perspective is that it gives a person the ability to recognize what errors are likely to occur when System 1 takes the wheel. Much of the time, these errors are harmless or inconsequential, but when the stakes are high, the intuitions of System 1 should not be taken at face value.

## Humans and Econs Are Models to Understand Real and Fictitious Economic Decision-Making

In Part 4, Kahneman's focus shifts to the consequences of the System 1 and System 2 dichotomy in economic decision-making. As in the rest of the book, he emphasizes the ways in which unquestioned System 1 intuitions can lead to dubious results. Economics is a particularly fertile field for this type of investigation because System 1 is poor at dealing with sums, statistics, and probabilities—all of which crop up frequently when one must make choices about money.

Borrowing the terminology of his colleague, economist Richard Thaler, Kahneman delineates two groups of economic agents: Econs and Humans. Econs are the fictitious, wholly rational buyers and sellers who seem to be implied by mainstream economic modeling. Their preferences are perfectly consistent, and their decisions are both selfish and logical. An Econ will never leave money on the table or pay an excessive premium to avoid risk and will almost never be found purchasing a lottery ticket. This is because Econs, generally speaking, reason about the _expected value_ of any situation they confront. To an Econ, a guaranteed $500 payment and a 50% chance to win $1,000 are equally attractive. Econs are also unaffected by the different ways in which a statement can be posed. Their preferences, as Kahneman puts it, are "states of reality" rather than states of mind. In choosing among those states of reality, an Econ will never be swayed by disappointment, regret, guilt, or the anticipation of any of these feelings.

Humans (in this context, the H is capitalized) are the ordinary, real-world people who actually make economic decisions. They are, needless to say, prone to a good deal of emotionality in their reasoning, and their decisions about chance events are often tinged by an exaggerated hope of gain or an exaggerated fear of loss. They feel regret, disappointment, and guilt when things do not work out, and they are conscious of the possibility of incurring these feelings whenever they consider a significant economic decision. The salience of the distinction—and its relevance to a book about thinking—comes from the fact that System 1 can be implicated in many of the "departures from rationality" that characterize Human economic behavior. The biases of System 1 are quite sensible when viewed from an evolutionary point of view: in a survival situation, it is generally more important to avoid a threat than to pursue an opportunity. When applied to economic thinking, however, these biases can lead to perverse outcomes and inconsistent, self-defeating choices.

## The Remembering Self Makes Decisions and the Experiencing Self Lives Out the Results

The last few chapters of _Thinking, Fast and Slow_ concern yet another sense in which the human mind is "a self divided." When making decisions, Kahneman observes, people are completely reliant on their memories of past events—they have no direct, unmediated access to the events themselves. Thus, he says, it is the _remembering self_ who makes decisions, as distinct from the _experiencing self_ who will live out the results of those decisions. Memory, like other aspects of cognition, has its own characteristic biases, and these drive a wedge of sorts between the remembering and experiencing selves. The duration of an experience, for instance, is almost completely ignored by the remembering self—a phenomenon Kahneman calls _duration neglect_. The most intense moments of the experience, whether painful or pleasurable, tend to loom large, as does any pain or pleasure at the very end of the experience. Together, these two traits constitute the _peak-end rule_, which has been observed in both experimental and observational studies.

Because of these biases, the remembering and experiencing selves are frequently at odds: the remembering self chooses the experiences that will make the best memories, not the ones that will be most enjoyable overall. People, Kahneman says "choose by memory," and thus they necessarily choose experiences based on their memories of peaks and ends. The experiencing self's drive to shorten pain and prolong pleasure, meanwhile, is not well represented in the remembering self's decision-making process. Instead, the remembering self is animated by the prospect of intense future pleasures, however brief, but unmoved by the notion of mild, prolonged pleasures. Conversely, the remembering self is willing to accept long episodes of unpleasantness, provided no single moment is too intensely unpleasant, but it will plan and strategize with gusto to avoid even transient moments of intense pain.
___
# Context

## Heuristics and Biases

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s early work is often described as the vanguard of _heuristics-and-biases_ _psychology_. This branch of psychological research, founded in the 1970s through the collaborations of Kahneman and Amos Tversky (1937–96), concerns the _heuristics_ or informal rules used by the human mind to make quick decisions. Three examples of such heuristics are laid out in Kahneman and Tversky's seminal 1974 paper, "Judgment Under Uncertainty: Heuristics and Biases." _Thinking, Fast and Slow_ presents a much more extensive list.

Much of the time, as Kahneman acknowledges in his writings, heuristics work well. The _representativeness heuristic_, for instance, is the tendency to judge the probability of an event based on how the circumstances match one's mental image of such an event. In many everyday situations, this is a reasonable and effective way to proceed. The likelihood of rain is, for example, indeed higher on a day that starts out cloudy (a day that "looks like rain") than on a day that begins with sunshine. Unfortunately, heuristics also leave people susceptible to various systematic errors in judgment—the "biases" in heuristics and biases. One known bias that accompanies the representativeness heuristic is _base-rate neglect_, in which people ignore statistical information and overestimate the likelihood of rare but easy-to-imagine scenarios.

Cognitive biases like the one just described are particularly pervasive when people attempt to reason about statistics or probabilities. Yet many of the judgments required in modern life—about finances, health care, politics, and so forth—are unavoidably statistical in nature. Thus, it is important to understand how people's intuitions operate (the heuristics they use) as well as how they fail (the biases they are susceptible to). Although Kahneman and Tversky pioneered this psychological research, many of their colleagues and students have made their own considerable contributions to the field. _Thinking, Fast and Slow_ cites dozens of such studies. The book is like an atlas of the inroads psychologists have made to understand human judgment.

## Behavioral Economics

The concept of cognitive heuristics and biases, along with Kahneman and Tversky's later work on prospect theory, found applications in a variety of social science fields. To Kahneman's own surprise, students of the theories included a small but enthusiastic group of economists. These economists used tools found within heuristics and biases research to conduct a systematic critique of traditional _rational-agent economics_. The mainstream economics of the 20th century bears this label because of its fundamental assumption that people make rational, self-seeking choices. More accurately, a rational-agent model assumes that prices and markets will behave _as if_ the people involved made rational, self-seeking choices.

The rational-agent model is informed by _game theory_, a branch of mathematical thinking concerned with the best way to make decisions under certain formal conditions. A tenet of many game theory scenarios is that an agent (e.g., a player in a formal game or a buyer in a market) will always act to maximize his or her own self-interest and will always reason correctly in doing so. Game theory dates back to the 1940s, but even then it was clear that human beings do not generally behave in such a consistent, predictable fashion. Still, as Kahneman has pointed out, it is more challenging to produce a theory that usefully accounts for these departures from rationality. In developing _prospect theory_, Kahneman and Tversky took on the challenge of accounting for irrational reasoning. They modeled and even quantified major aspects of the "irrationality" of human decision makers. American economist Richard Thaler (b. 1945) worked with Kahneman and Tversky in the late 1970s. Thaler went on to spearhead the development of the discipline of _behavioral economics_, which sought to reconcile economic modeling with the insights derived from modern psychology. The field has grown explosively over the past 40 years. Behavioral economics are the subject of many popular works, including Thaler's own books _Nudge_ (2008, with Cass Sunstein) and _Misbehaving: The Making of Behavioral Economics_ (2015).

The reader of a typical first-year microeconomics text might get the idea that economists are blind to the limitations of the rational-agent model. This is hardly the case. For one thing, introductory texts offer a much more simplified picture of economic behavior than that used by researchers. Moreover, despite its seemingly odd assumptions about the individual, the rational-agent model often works well for describing human behavior in aggregate. It gives highly accurate predictions in a few scenarios and viable "ballpark" predictions in many others. The adoption of behavioral principles in mainstream economics has been slow and uneven, but not because economists truly believe human beings are rational, self-seeking logic machines. Rather, the debate over when and how to apply behavioral ideas concerns a trade-off: complexity versus accuracy. As Kahneman explains in _Thinking, Fast and Slow_, a more sophisticated theory—of economic choice or any other decision-making behavior—can only be justified if the benefits outweigh the increased complexity. Some economists, like Thaler, see considerable value in making such a trade. However, others maintain that the rational-agent model offers an acceptably accurate account of economic behavior, and that behavioral principles unjustifiably complicate the picture.

## Challenges and Criticisms

Kahneman's assertions in _Thinking, Fast and Slow_ are built not just on his own experiments, but on a wider body of experimental evidence collected by many psychologists. However, in the early 2010s psychology researchers—along with other social science and life science researchers—were increasingly criticized when their experiments were repeated with contradictory or inconsistent results. Whether the experiment was repeated by the original researchers or by colleagues, the outcome in hundreds of cases was that the original findings could not be reproduced. In some cases, the inconsistency was blamed on poor experimental design or on dubious research practices. In other cases, critics concluded that the original investigators had simply overstated the statistical validity of their results. This situation came to be known as the _replication crisis_ or _reproducibility crisis_. Though this crisis is most closely associated with the years 2011–15, scholarly articles continued to discuss it in 2019.

The _anchoring effect_ is one of Kahneman's direct contributions to the field of heuristics and biases. It assumes that decision-making is influenced by having an anchor, or focal point. For example, if a person reads online that a car costs a certain amount, then finding the same model for sale at a lower price will increase the likelihood of the person buying the car—even if further research would have discovered an even lower price. Unfortunately, several of the studies conducted by other researchers and cited by Kahneman were implicated in the replication crisis. Studies of _priming effects_, in which a subliminal or otherwise unnoticed cue alters a person's behavior and cognition, were among the hardest hit by the crisis. Follow-up studies failed to find support for the results of several landmark experiments that shaped this research.

In an article titled "The Irony Effect" for _Slate_ magazine, journalist Daniel Engber surveys the ways in which the replication crisis affected Kahneman's work—particularly the claims made in _Thinking, Fast and Slow_. To an extent, Engber faults Kahneman for overemphasizing the implications of research that later turned out to be unreproducible. "The scientist who founded the science of mistakes," Engber observes, "ended up mistaken." Nonetheless, Engber is careful to note that Kahneman's work has stood up to the reproducibility test. In other words, later experiments of similar design have largely corroborated Kahneman's own research. Even so, Kahneman did not passively accept the replication crisis or the erosion of confidence in cognitive psychology that the crisis caused. Instead, he urged researchers in the field of priming theory to organize a system of mutual oversight, with researchers closely following and attempting to replicate colleagues' work. Insofar as it is adopted, this practice would ensure that any new priming effects reported are more robustly backed by data from multiple studies.
___
# Summary

## Summary

_Thinking, Fast and Slow_ describes the shortcuts and pitfalls of human cognition in terms of three pairs of traits. The first three of the book's five parts deal with the distinction between Systems 1 and 2, psychological shorthand for "fast, intuitive thinking" and "slow, deliberate thinking." In Part 4, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) draws a further distinction between _Econs_, the rational agents envisioned by traditional economics; and _Humans_, the real human beings whose choices seldom perfectly conform to rationality. Part 5 contrasts the _experiencing self_ and the _remembering self_, showing how choices about future experiences are driven by imperfect recollections.

### Part 1: Two Systems

Kahneman begins by presenting a "two-systems" view of human cognition. For the most part, he asserts, people make use of automatic, intuitive, and largely unconscious thinking to arrive at judgments and make decisions. Kahneman labels this style of thinking System 1. It is distinct from the more effortful and deliberate System 2. Because it is costly—and in fact physiologically demanding—to use System 2, the great majority of everyday choices are made on the basis of System 1 reasoning, with little oversight from System 2. It is much more common for the "lazy" System 2 to accept and endorse the System 1 suggestions than to override or question them.

One trait of System 1, Kahneman says, is its reliance on _cognitive ease_ as a proxy for truthfulness. Anything that puts less strain on the mind, or requires less cognitive effort to untangle, is likely to come across not only as pleasant but also as true or correct. It is harder for a person to exercise intuition if he or she is strained and preoccupied. Conversely, a happy and relaxed mood makes it easier to accept intuitions as facts and forgo more deliberate forms of reasoning.

One major contributor to cognitive ease is _coherence_, which System 1 instinctively seeks out. System 1 specializes in creating compelling stories to explain events, with distinct characters and a recognizable pattern of cause and effect. Statistical reasoning, in which the comforts of coherence are often absent, is difficult and unappealing by comparison with causal reasoning. It also requires the extensive involvement of System 2. The net effect of the two systems is that the mind is typically overconfident in answers hastily generated by System 1, and it is reluctant to second-guess the answers by calling on System 2.

### Part 2: Heuristics and Biases

With Systems 1 and 2 as the backdrop, Kahneman proceeds to an exploration of the _heuristics and biases_ that drive human cognition. _Heuristics_, he explains, are intuitive rules of thumb for solving problems in a quick and approximate fashion. In much of day-to-day life, reliance on these (usually unconscious) rules will produce an acceptable result. Nonetheless, heuristics are not the same as a thorough, detailed System 2 consideration of a question or decision. In fact, heuristic reasoning is prone to systematic _biases_, unreasoned attitudes or judgments. Biases can be magnified when a person attempts to apply his or her intuition to large numbers, rare cases, or complicated financial choices.

Kahneman describes three heuristics in detail:

- The _availability heuristic_ is the tendency to estimate an event's frequency by how easily one can come up with examples.
- The closely related _representativeness heuristic_ is the tendency to assess how "typical" or "representative" a situation is as a proxy for its frequency.
- The _anchoring heuristic_ is the tendency, when estimating quantities, to start with a number that comes easily to mind—an _anchor_—and then adjust upward or downward. People tend to stick quite close to the anchor value, even if this value has no basis in fact.

  

The net effect of all three heuristics is that people tend to overestimate the likelihood of extreme events, making catastrophes and windfalls loom larger in their minds.  

### Part 3: Overconfidence

The mind's trust in the intuitions of System 1 leads not just to flawed judgments, but to overconfidence in those judgments. Kahneman describes this overconfidence in terms of a series of _cognitive illusions_ to which the mind is prone. The _narrative fallacy_, for instance, is at work when a person claims to have "known all along" something would happen when by its nature it could not have been predicted. The _illusion of validity_ and the closely related _illusion of skill_ show themselves when people place their faith in judgments—for example, a stock's future value—that produce results no better than random chance.

In many cases, Kahneman suggests, society would be better off entrusting supposedly "expert" judgments to algorithms. However, there is widespread hostility to this idea. There are, Kahneman concedes, some cases in which genuine expert intuition can be shown to exist—a chess grandmaster's ability to size up a board, for instance. However, this happens only within relatively structured, regular environments in which feedback is clear and instantaneous. Kahneman proposes that it is important for a person to take the "outside view" of any course of action in order to avoid getting caught up in the illusion of his or her own predictive powers. He suggests a person should focus on overall statistical tendencies, rather than on what he or she hopes will happen.

### Part 4: Choices

In Part 4 Kahneman shifts gears to a discussion of _behavioral economics_. His characters in this part are not System 1 and System 2, but _Humans_ and _Econs_. The word _Econ_ is a label for the simplified human being imagined by mainstream economic theory—a logical, rational creature with a perfectly consistent set of preferences. In contrast, _Humans_ are the buyers, sellers, workers, and employers of the real-world economy. Humans' preferences are often inconsistent, and their behavior is seldom strictly rational. Kahneman points out some of the flaws of an economic model based on purely rational agents. He then explains his own efforts to account for Humanlike patterns of economic decision-making. He describes some simple, easily modeled ways in which Humans clearly differ from Econs, such as the Human tendency to be risk averse when losses are involved but risk seeking when faced with the prospect of gain. Humans, he observes, are prone to a _mental accounting_ in which "losses loom larger than gains."

### Part 5: Two Selves

In the last four chapters, Kahneman sets out to explain some flaws in human decision-making when time is taken into consideration. He posits a model of the individual as two complementary "selves," which he labels the _experiencing self_ and the _remembering self_. The experiencing self goes through the day-to-day pleasures and pains of life. When those experiences are remembered, however, a different—in fact, systematically distorted—perspective takes charge, and decisions about the future are made on the basis of those memories. The remembering self overemphasizes the intensity of experiences while discounting their duration. This sometimes leads to demonstrably bad choices from the perspective of the experiencing self.

Because the experiencing and remembering selves have different priorities, there is no one such thing as "happiness." Instead, things that lead to experienced well-being in the moment will often have a diminished or opposite effect on assessments of overall life satisfaction. A good experience does not necessarily translate to a happy memory, or vice versa. In attempting to define and measure happiness, Kahneman suggests, psychologists and policymakers must incorporate the testimony of both the experiencing and the remembering self. Generally, the two will not tell the same story.
___
#  Part 1, Introduction–Chapter 2 : Two Systems | Summary

## Summary

### Introduction

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) begins by describing the goal of his book: to give people a richer vocabulary for discussing and detecting errors in judgment. He offers a brief history of his own professional interest in the psychology of judgment and decision making, illustrated by some examples of the successes—and the failures—of human intuition. Finally, Kahneman provides a high-level outline of _Thinking, Fast and Slow_, which begins by detailing the workings of two complementary "systems" of cognition and describes the heuristics, or rules of thumb, on which those systems rely. In the "Origins" section of the Introduction, Kahneman discusses his research and thought partner, the late Amos Tversky, at length. Tversky's contributions were central to all of Kahneman's work and success.

### Chapter 1: The Characters of the Story

Kahneman introduces the two systems alluded to in the introduction. System 1 is the automatic, intuitive set of thought processes by which people often make decisions, sometimes without conscious awareness. System 2 is deliberate and rational, but it is also lazy, often superficially endorsing whatever intuitive judgment System 1 comes up with. Moreover, the use of System 2 involves a focusing, and thus a narrowing, of attention. When System 2 is engaged on a problem, a person becomes much less aware of anything not immediately relevant to solving the problem. The systems come into conflict whenever a person must do something counterintuitive, such as "steer into the skid" on an icy road or make sense of seemingly contradictory visual data.

### Chapter 2: Attention and Effort

System 1, Kahneman says, is "at the center of the story," whereas System 2 is a mere "supporting character." This is in part because System 2 usually eschews effort, only getting involved in a decision if and insofar as it needs to. To give the reader a reference point for System 2 exertion, Kahneman describes, and invites the reader to try, a simple but challenging arithmetic exercise called the _Add-1 task_. In experiments, he says, performance of the Add-1 task has been shown to render people "effectively blind" to irrelevant stimuli. Mental effort—the intense and sustained engagement of System 2—also produces a suite of physiological effects, including dilated pupils and an elevated heart rate.

## Analysis

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s introduction begins with an apparently pessimistic tone, not the "can-do" motivational rhetoric one might expect from a popular psychology book. People, he suggests, are not very astute self-critics, especially when it comes to their own reasoning processes. They are, however, quite adept at (and in many cases fond of) criticizing others. Thus, Kahneman holds out the hope that readers of his work can become more skilled at this sort of criticism, helping to detect the blind spots in other people's decision making. In this sense, the work is not as pessimistic as it may first appear—cognitive self-awareness simply needs to be recast as a collective or communal enterprise. Kahneman's mission to improve the quality of "gossip" is a roundabout way to improve society's overall quality of thinking.

At the end of Chapter 1, Kahneman is careful to point out that the two "systems" are really aliases for different ways of thinking. They are not distinct physical regions of the brain, separate neural pathways, or anything quite that concrete. In part, Kahneman chooses the names System 1 and System 2 because these terms have some currency in other psychological literature. More pragmatically, he adopts them because they are memorable and concise, two traits that lie at the heart of many cognitive heuristics having to do with language. He treats System 1 and System 2 as characters in a story because, as later chapters will underscore, a story with characters is much more easily understood and assimilated than a non-narrative exposition of dry facts. The structure and vocabulary of _Thinking, Fast and Slow_ embody some of its author's fundamental insights about learning and attention.

One of the optical illusions presented in Chapter 1, the Müller-Lyer illusion, has a long and interesting history of its own. This is the famous illusion of the two arrowlike figures, one with two heads pointing out (<—>) and one with two tails (>—<). The lines themselves are identical in length, but almost everyone perceives the line in the two-headed arrow (<—>) as shorter than the corresponding line in the two-tailed one (>—<). This perception is remarkably hard to shake even if one is familiar with the illusion. The effect was first documented by German psychiatrist Franz Carl Müller-Lyer (1857–1916) in 1889 and has been the subject of many explanatory efforts. Some theories emphasize the fact that human vision evolved to deal with three-dimensional environments, making it easily fooled by flat images. Other explanations rest on the idea that the brain's first, unconscious judgment of length is not sophisticated enough to distinguish the arrow shaft from its tails.

For Kahneman's purposes, the Müller-Lyer illusion is noteworthy mainly because it shows the persistence and automaticity with which System 1 can make a mistake. No matter how many times one views the illusion, it is still likely to seem as though one line is shorter than the other. It takes conscious restraint—the exercise of System 2—to recognize that one's intuition is making a systematic error in judgment.
___
# Part 1, Chapters 3–5 : Two Systems | Summary

## Summary

### Chapter 3: The Lazy Controller

Complex, methodical System 2 thinking demands self-control, particularly if such thinking is performed under time pressure. [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) notes some exceptions to this general trend: in a pleasant and well-studied psychological state called _flow_, all one's attention goes into the activity at hand, and no effort is needed to stick to the task. Still, in general, being _cognitively busy_ leaves one with less willpower to resist such temptations as junk food or impulsive spending. This state of diminished self-control is known as _ego depletion_. Keen to avoid such an expenditure of willpower, System 2 seldom contradicts the intuitions of System 1 unless an obvious discrepancy prompts further investigation. Children who possess a strong and unusually active System 2, who are capable of delaying gratification and exercising self-restraint, are likely to perform better on intelligence tests later in life.

### Chapter 4: The Associative Machine

System 1, Kahneman suggests, works largely by association. Once an idea has been "activated"—for instance, by reading a word—System 1 spontaneously searches for related and compatible ideas. Much of this associative work happens unconsciously, as can be observed in studies of so-called priming effects, patterns of behavior and cognition that appear when a subject is primed with a particular stimulus. Students asked to solve a crossword puzzle featuring elderly themed words (e.g., "gray" or "wrinkle") will move more slowly when walking down the hall afterward. Kahneman cites several other studies in which a seemingly innocuous or irrelevant stimulus produced a conceptually related effect on a subject's thoughts or actions.

### Chapter 5: Cognitive Ease

Next, Kahneman expands on a concept discussed briefly in Chapter 3. In deciding whether System 2 should be tapped to evaluate a decision, he says, System 1 relies on a perception of _cognitive ease_ or its opposite, _cognitive strain_. The more strained System 1 is, the less effective its intuitions are, and the more likely System 2 is to be called in to consciously address the problem at hand. The causes of cognitive ease, however, include some wholly incidental features that have no bearing on whether a problem is easily solved by intuition—or whether a given proposition is likely to be true. Presenting a statement in a clear, bold font, for instance, makes it seem more familiar and less cognitively burdensome, triggering System 1 to see the often incorrect intuitive answer as more plausible. However, problems presented in small, difficult-to-read font activated System 2 and led more participants to reject the incorrect intuitive answer suggested by System 1 and to arrive at a correct answer by using System 2.

The rhyme-as-reason effect is another example of how cognitive ease can mislead: rhyming sayings are "judged more insightful" than phrases with near-identical meanings that do not rhyme. Mere exposure to just about any stimulus, provided it is not noxious, can lead a person to later associate that stimulus with feelings of familiarity and ease. When influenced by such feelings, an individual will rely even more heavily than usual on System 1, whether or not this reliance is warranted by the nature of the cognitive problem to be solved.

## Analysis

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) mentions the concept of flow only in passing, perhaps because his focus is on the ordinary human experiences in which distractions abound and paying attention is effortful. The study of flow belongs to a broader domain called _positive psychology_, which investigates the psychological underpinnings of happiness and well-being. In _Thinking, Fast and Slow_, Kahneman, though by no means a pessimist, is more interested in the ways that otherwise successful intuitions can fail.

Nonetheless, the literature on the topic of flow is extensive, dating back to about the same time as Kahneman's own early studies on heuristics and biases. The founding figure in this domain of research is Mihaly Csikszentmihalyi, a Hungarian American psychologist whose theories are drawn from extensive, worldwide interview and survey data. Csikszentmihalyi proposes the existence of a cognitive sweet spot of sorts, in which a person's skills are perfectly matched to the challenges they face. In this respect, Csikszentmihalyi's work contrasts notably with Kahneman's, which tends to unearth _mis_matches between humankind's cognitive tools and the problems it must solve. Csikszentmihalyi first introduced his ideas to a popular audience in _Beyond Boredom and Anxiety_ (1975), but his best-known book is _Flow: The Psychology of Optimal Experience_ (1990). Nearly three decades after it was first published, _Flow_ continues to be widely read and is a frequent set text in psychology, design, and cognitive science courses.

Willpower is another concept worthy of a deeper dive. The lack or exhaustion of willpower—what Kahneman and others call _ego depletion_—can intensify cognitive biases and suppress System 2 thinking. Willpower has also, somewhat more controversially, been proffered as an explanation for the vastly different academic outcomes among children with similar educational backgrounds. Toward the end of Chapter 3, Kahneman cites a classic study of willpower in young children: the famous "marshmallow test" or the "Stanford marshmallow experiment" conducted by Walter Mischel. Kahneman describes a version of the study that involved Oreos instead, but regardless of the snack chosen, the basic experimental protocol was the same. The experimenter would offer a child one marshmallow (or cookie, pretzel, etc.) and promise to double the reward if the child could wait 15 minutes. To endure the wait, the children often attempted to distract themselves or to ignore the marshmallow by turning around or covering their eyes. About a third of the subjects managed to wait the full 15 minutes without eating the marshmallow. As a group, these children were found to have better educational outcomes later in life—as measured, for instance, by standardized test scores. Some aspects of the "marshmallow test" experiment and its interpretation have since been called into question. In a June 2018 article for _The Atlantic_, sociology professor Jessica McCrory Calarco suggests that home environment, rather than innate willpower, may play the main role in explaining why some children can delay gratification and others cannot. However, Mischel has continued to endorse the willpower explanation—for example, in his popular book _The Marshmallow Test_ (2014).
___
# Part 1, Chapters 6–7 : Two Systems | Summary

## Summary

### Chapter 6: Norms, Surprises, and Causes

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) explains that System 1 is constantly engaged in "maintain[ing] and updat[ing] a model" of the world, and of "what is normal in it." Against the backdrop of these mental norms, some events stand out as surprising, but the mind quickly adapts to surprises and fits them into the overarching pattern. Thus, an event System 2 knows to be rare can seem familiar and expected to System 1 simply because it matches up with a past experience. Faced with two consecutive surprises, System 1 works to weave them together into a pattern that makes neither event surprising. In doing so, System 1 often comes up with causal explanations, in which agency, blame, and intention are imputed even to inanimate objects. Such causal reasoning works adequately much of the time, but it works against the grain of any attempt to reason statistically. When phenomena have no clear single cause, or must be considered in aggregate, statistical reasoning (a System 2 specialty) is necessary.

### Chapter 7: A Machine for Jumping to Conclusions

The net effect of many System 1 intuitions is a tendency to jump to conclusions. "Conscious doubt" and the toleration of uncertainty require the deliberate, effortful engagement of System 2. Without such conscious scrutiny, people are prone to _confirmation bias_, in which evidence that fits into preexisting beliefs is given more weight than contradictory evidence (which may be dismissed entirely). This bias can take the form of a _halo effect_, a "tendency to like (or dislike) everything about a person."

Fundamentally, Kahneman says, these biases spring from the fact that System 1 is tasked with fitting available information into a coherent story—not with seeking out more information to challenge the story or fill in gaps. Kahneman gives a colorful name to this basic cognitive tendency: "What You See Is All There Is," or WYSIATI for short. In later chapters, Kahneman will refer back to WYSIATI as a causal factor in many different types of biases.

## Analysis

One way to get a sense of the System 1/System 2 dichotomy is to think of the classic video game _Tetris_, in which players must fit together a series of falling blocks. System 1 is like a novice _Tetris_ player, concerned only with the blocks currently on screen. Its goal is to take the information in front of it (the blocks) and fit that information into as consistent and plausible a pattern as possible. Because order and simplicity are its overriding goals, System 1 recoils from anything that doesn't fit; more "blocks" are not necessarily better from its point of view. More information is welcome if it fits nicely into the preexisting pattern (in cognition, this is called _confirmation bias_) and is decidedly _un_welcome if it disrupts the pattern.

In fact, anyone who has played _Tetris_ also has a good visual model for _cognitive ease_ and _cognitive strain_, two concepts introduced in Chapter 5 and repeated throughout the book. The slow pace, few blocks, and ample maneuvering room at the beginning of the game create a condition of cognitive ease. The neatly fitted blocks symbolize the mental state on the screen. As the pace quickens, blocks pile up, and things get more difficult to fit together, cognitive strain sets in. The haphazardly stacked blocks are a virtual mirror image of a mind struggling to integrate large amounts of potentially contradictory information.

An expert _Tetris_ player does not just think one block at a time. Patterns are imagined in advance, contingencies are planned for, and opportunities are identified. Thinking about what blocks _might_ show up, however, is a System 2 activity—both in _Tetris_ and in life. The saying "out of sight, out of mind," though accurate enough as a general observation about human cognition, is overwhelmingly true of System 1.
___
# Part 1, Chapters 8–9 : Two Systems | Summary

## Summary

### Chapter 8: How Judgments Happen

System 1, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) asserts, understands the world in terms of _basic assessments_—simple, approximate, intuitive readings of the current situation. These assessments tend to sort events and objects into crude categories: aversive or attractive, threat or opportunity. A stranger on the street, for example, is instantly and unconsciously assessed as either "friendly" or "hostile" based on physical build, facial expression, and so forth. However, there are some rather severe limits to the problems that can be solved by such assessments. System 1 is good at estimating averages, for instance, but very poor at estimating sums, or what Kahneman calls _sum-like variables_. Unfortunately, statistical and probabilistic reasoning rely on the ability to compare such variables.

From the point of view of System 1, an easier task is _intensity matching_, in which corresponding values must be assigned to two variables with different dimensions. The seriousness of a crime, for example, can be intuitively matched to the severity of the punishment, just as the loudness of a noise can be matched to the brightness of a color. Like other traits mentioned in this chapter, the tendency toward intensity matching simplifies everyday decision making but greatly complicates the task of attempting to think statistically.

### Chapter 9: Answering an Easier Question

A heuristic can be thought of as a way of substituting a more easily answered question for the one being asked. Kahneman calls these the _heuristic question_ and the _target question_, respectively. For example, if the target question is "How happy are you with your life these days?" many people base their answer on the heuristic question "What is my mood right now?" This specific substitution is an example of the _mood heuristic_, in which people rely on an assessment of their current mood as a "shortcut" to answering a much more challenging question. It is easy and tempting to simply ask "How do I feel about it?" rather than weigh the pros and cons of a situation or a course of action.

## Analysis

The mood heuristic pervades many aspects of social, economic, and political life. [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) focuses on its implications for electoral politics. A candidate whose appearance inspires _feelings_ of confidence, he observes, is likely to be seen as more electable than an unprepossessing candidate with a more impressive track record. Aware of this fact, campaign strategists often go one step further and attempt to tell the public how a candidate's looks _should_ make them feel. An example of this occurred in the runup to the 2004 U.S. presidential election, when the associates of then-president George W. Bush tried such a strategy with respect to Democratic challenger John Kerry. "He looks French," said Commerce Secretary Don Evans, evidently attempting to yoke Kerry's physical appearance to anti-French sentiment at the onset of the Iraq War. This was the same era of U.S. political history that produced the euphemism "freedom fries." Little effort was expended to explain just what Kerry's facial features and mannerisms had to do with France.

Product marketing is another area in which the mood heuristic is widely deployed. Witness, for instance, the extensive holiday advertising efforts undertaken by Coca-Cola, Folger's, and other consumer brands. In producing costly, high-profile advertisements featuring Santa Claus and Christmas lights, these companies are not merely hoping to boost sales for the few weeks in December when such ads are usually run. Rather, they are cultivating positive emotions around their products with the aim of promoting brand loyalty year-round. They are attempting to create what Kahneman previously called a _halo effect_ (Chapter 7), in which a product's positive associations (good times, holiday cheer) ripple outward to influence impressions of more relevant features, such as taste or effects on health. Indeed, it's difficult to find a soft drink advertisement that says anything of substance about the actual product, and hard _not_ to find a soft drink advertisement that appeals to the mood heuristic instead.
___
# Part 2, Chapters 10–12 : Heuristics and Biases | Summary

## Summary

### Chapter 10: The Law of Small Numbers

Because System 1 tends to reason causally about events, it is easily fooled by small samples reporting extreme results. It is a basic statistical truth that small samples are more likely to display extreme outcomes: one is much more likely to see "all heads, no tails" when flipping four coins at once than when flipping eight. System 1's inability to account for this fact is what [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) and Tversky wryly termed "the law of small numbers."

### Chapter 11: Anchors

Next, Kahneman introduces the _anchoring effect_, in which people are found to be extremely suggestible in making numerical estimates. The discovery of such an effect is one of Kahneman and Tversky's most important joint contributions to the psychological literature. Exposure to a number—even one known to be randomly chosen—will influence a person's estimate of the height of a redwood tree, Gandhi's age at death, or the year George Washington was inaugurated. In each case, the respondent has some information: redwoods are very tall, Gandhi was old—but not hundreds of years old—when he was assassinated, and George Washington could not have become president before 1776. Yet a respondent primed with a high number ("Was Gandhi more or less than 144 years old when he died?") will still give a higher estimate than one primed with a low number. This is similar to the _anchor-and-adjust_ heuristic, in which an anchor is chosen early in the reasoning process, and any additional information is used to make slight adjustments.

### Chapter 12: The Science of Availability

The _availability heuristic_ is one well-studied means by which System 1 estimates frequencies. To decide how frequent or likely something is, people often rely instead on how easy (or difficult) it is to think of examples. This heuristic is at work when, for instance, spouses overestimate their own contributions to household chores, so that the total of the two estimates is greater than 100%. Examples of chores one has done oneself are easier to come up with than chores done by one's partner. The impression of cognitive availability can itself be manipulated by asking for more or fewer examples. People asked to list 12 examples of their own assertive behavior have a hard time filling the list, and often come away with the impression that they are not very assertive after all.

## Analysis

The "law of small numbers," introduced in Chapter 10, is named as a riff on the _law of large numbers_, an actual theorem from probability theory. The law of large numbers states that the more often a random event is repeated, the more consistent the average result will be. If one bets on a single number—say, 7—on a roulette wheel, then one will either win or lose with every individual spin. Although there are 38 spaces on a roulette wheel, a win and a loss are the only possible outcomes of a given spin from the bettor's point of view. With a hundred or a thousand such spins, however, the win/loss ratio of observed outcomes will gradually approach 1:38, the ratio of the actual probability of landing on 7 each time the wheel is spun. Interestingly, as [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) points out in Chapter 10, even trained professionals design experiments with inadequate sample sizes that they determine based on their judgment. This reality is part of the root of the reliability or replication crisis.

Both the availability heuristic (Chapter 12) and the so-called anchor-and-adjust heuristic (Chapter 11) fit into the pattern Kahneman described in Chapter 9 ("Answering an Easier Question"). In a typical anchor-and-adjust scenario, a person begins with a difficult question that invites, or even requires, a precise answer ("How much money should I save per month for retirement?") The question is made much easier if one has an anchor to grasp onto, even if the anchor is an arbitrary and artificial number. A similar process is at play when availability is substituted for frequency. Figuring out the prevalence of bears in a certain national forest, for example, might require considerable research. The question "Do I know of anyone who has seen a bear in this forest?" is far easier to answer.

The availability heuristic also has implications for medicine, both epidemiologically and in clinical practice. In the United States, for instance, people frequently overestimate the likelihood of a spider bite being poisonous because the names of two highly poisonous spiders—the black widow and the brown recluse—are familiar to many Americans from childhood onward. Species of nonpoisonous spider, if learned at all, understandably make less of an impression and are less "available" when one thinks of a spider bite. Likewise, many rare conditions are highly "available" in the minds of the public because such conditions are featured on such TV shows as _House_, _ER_, and _Grey's Anatomy_. In Chapter 12, Kahneman describes the opposite effect: people display a casual attitude toward disease prevention when the disease in question is not highly salient. People with no family history of heart disease will underestimate the likelihood of developing it themselves. People whose friends do not get an annual flu shot are less likely to decide to do so themselves.
___
# Part 2, Chapters 13–15 : Heuristics and Biases | Summary

## Summary

### Chapter 13: Availability, Emotion, and Risk

Here, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) fills out the previous discussion of the _availability heuristic_. Judgments of availability, he says, are skewed by media coverage, which is "itself biased toward novelty and poignancy." People are much more likely to hear about a fatal accident or a homicide in the news than they are about a death attributable to diabetes or asthma. Consequently, people exaggerate the likelihood of events they have come to fear while downplaying the likelihood of events that get less media attention. It follows that when experts and policymakers attempt to quantify _risk_, they may initiate an _availability cascade_. Biases come to be magnified in the public imagination, making it harder for moderate voices or conflicting information to be heard.

### Chapter 14: Tom W's Specialty

Kahneman now introduces the _representativeness heuristic_ and the related concept of a _base rate_. The representativeness of an event or person is the degree to which they seem typical ("representative") of a category. A preference for "neat and tidy" environments, for example, is often seen as representative of librarians. The heuristic aspect lies in people's tendency to trust representativeness over, or instead of, pertinent statistical information. Kahneman gives the example of an experiment in which he and Tversky asked respondents to guess the major of a graduate student named Tom W. The brief synopsis of Tom's personality made him seem representative of computer scientists, and most respondents guessed Tom's field of study accordingly. In doing so, they ignored the base rate: the small proportion of graduate students in computer science as compared to other fields.

### Chapter 15: Linda: Less Is More

The representativeness heuristic, however, does not merely distort probabilistic estimates: it can sometimes lead people to make completely illogical guesses. In the famous "Linda" experiment of the 1980s, Kahneman and Tversky told subjects about a young woman named Linda who "is thirty-one years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations." After reading this short descriptive paragraph, subjects were asked to determine which was more likely: "Linda is a bank teller," or "Linda is a bank teller and is active in the feminist movement." Overwhelmingly, but illogically, respondents chose the latter—a tendency Kahneman calls the _conjunction fallacy_. The chapter concludes with a survey of other studies demonstrating this intuitive but erroneous "less-is-more" thinking.

## Analysis

In describing the political effects of the availability heuristic, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) cites the work of two other scholars in the field, legal scholar Cass Sunstein and psychologist Paul Slovic. Sunstein and Slovic have nearly opposite stances on the role to be played by experts in assessing and communicating risk. Sunstein, popularly known for his book _Nudge_ (cowritten with Richard Thaler, 2009), argues that experts should carry out their analyses as objectively as possible. He says they should always avoid succumbing to pressure from an often uninformed and emotional public. Slovic, who specializes in the study of risk, asserts that (to use Kahneman's phrase) "the public has a richer conception of risks than the experts do." Further, Slovic argues that the fears experienced by the public are themselves a form of suffering and should be taken seriously, even for that reason alone. An exposition of Slovic's ideas can be found in _The Irrational Economist_ (2010) and _The Feeling of Risk_ (2013). Kahneman, who has collaborated professionally with both men, diplomatically avoids pronouncing a "winner" in the debate.

The _conjunction fallacy_, illustrated in Chapter 15, deserves some further unpacking. The reason so many people get the Linda problem wrong lies in the tendency to conflate _plausibility_ with _probability_. As Kahneman observes, this is an easy mistake to make, but the two concepts are logically distinct. "Linda is a bank teller and is active in the feminist movement" makes for a more plausible story than "Linda is a bank teller," but the probability of "bank teller" must be higher—or at least no lower—than the probability of "bank teller and feminist." The combination of two criteria—in this case, bank teller and feminist—is known as a logical conjunction.

The difference can be easier to see if the problem is posed in numerical terms. Suppose there are 80 bankers in the city where Linda lives. Then some subset of those 80 people are also feminists. It may be the case that all 80 bank tellers are feminists; it may be the case that none of them are. However, one thing is clearly impossible: if there are 80 bank tellers, there cannot be more than 80 feminist bank tellers. The statement "90 out of 80 bank tellers in the city identify as feminists" is readily recognized as absurd within a given population: there cannot be more feminist bank tellers than there are bank tellers overall. It follows that the probability of Linda being a feminist bank teller must be no higher than her probability of being a bank teller in the first place.
___
# Part 2, Chapters 16–18 : Heuristics and Biases | Summary

## Summary

### Chapter 16: Causes Trump Statistics

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) now draws a further distinction between _statistical base rates_ and _causal base rates._ Statistical base-rate information is framed in broad terms: "85% of the cabs in the city are Green" Cab Company. Causal base-rate information is more specific and seems, in its presentation, to suggest a direct connection to the individual case at hand: "Green cabs are involved in 85% of accidents." When both types of base-rate information are present, the statistical sort is much more likely to be thrown out or underweighted, because System 1 has trouble fitting it into a narrative. More depressingly—in Kahneman's view, at least—people "quietly exempt themselves" from statistics that fail to accord with their self-image.

### Chapter 17: Regression to the Mean

The concept of a "jinx" is widespread in sports. If an athlete has an outstandingly good first day at a tournament, she is expected to perform less well on day two. The sportscasters covering the event will advance all sorts of causal explanations for the drop in performance: the athlete was nervous because of the higher expectations, she was exhausted from an unusually strenuous effort on day one, and so forth. What's really going on, Kahneman says, is somewhat less sensational. Almost any set of outcomes, from test scores to inches of daily rainfall, will follow a distribution in which extreme events are rare and "average" events are common. Thus, there is a strong statistical tendency for any extreme event to be followed by a less extreme one, a phenomenon called _regression to the mean_.

### Chapter 18: Taming Intuitive Predictions

Understanding regression to the mean allows one to identify, and correct, predictions that do not take such regression into account. An extremely tall child is statistically likely to become a rather (but not extremely) tall adult. An extremely precocious kindergartener is likely to become a high achiever in college, but not to the same extreme level witnessed in early childhood. In each case, the statistically favorable outcome lies between the one piece of extreme information (height or intelligence at one point in time) and the mean, to which some regression is expected. Extreme predictions may be exciting—they capture the imagination, and getting them right is very gratifying—but they are seldom correct. Where accuracy is important, it pays to temper one's predictions to account for regression.

## Analysis

The "disheartening" results [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) cites in Chapter 16 come from the work of social psychologists Richard E. Nisbett and Eugene Borgida. Several of Nisbett's later studies and publications follow up on a theme closely akin to Kahneman's assertions that "causes trump statistics." In _The Person and the Situation_ (1991), cowritten with Lee Ross, Nisbett argues for a view of social behavior known as _situationism_. From this perspective, behavior in any setting is assumed to depend largely on the situation itself, rather than on a person's inherent disposition or personality. Some support for situationism can be found in the experiment mentioned in this chapter: left alone with someone who is having a seizure, most individuals would try to offer assistance or at least call for medical help. However, each of those same people will be much less likely to take action if there are several people who could offer aid. The change in situation overrides any individual disposition to offer help.

The connection back to Kahneman lies in the fact that situational factors lend themselves to being described in statistical terms: "only 20% of people faced with this problem solved it correctly." Dispositional factors, meanwhile, are naturally viewed as more inherent to a person—they tend to be understood as causal, not statistical, information. To assert that people overemphasize dispositional traits and underemphasize situational ones (as Nisbett does) is closely akin to saying (as Kahneman proposes) that people would rather see themselves as individuals than as statistics.

However, popular works by Nisbett seem to challenge Kahneman's half-joking claim that "teaching psychology is mostly a waste of time." In _Mindware: Tools for Smart Thinking_ (2015), Nisbett attempts a survey of cognitive psychology similar in some respects to Kahneman's _Thinking, Fast and Slow_. The books differ considerably, however, not just in the examples they cite, but in their overall tone and orientation. Nisbett explicitly presents principles from psychology and philosophy as "tools" the reader can learn to utilize. Kahneman, much more cagily, positions _Thinking, Fast and Slow_ as a book to enrich the vocabulary of "water cooler" gossip. His modest hope—so he says—is to make the criticism of inevitable bad decisions more eloquent and insightful.
___
# Part 3, Chapters 19–20 : Overconfidence | Summary

## Summary

### Chapter 19: The Illusion of Understanding

System 1, as previous chapters have suggested, tends to try to reduce experiences into a cogent, coherent narrative. This tendency carries with it the risk that the past will come to seem inevitable, that the narrative will seem as though it could not have played out any other way. People often come to believe they knew all along what would happen in an inherently unpredictable situation, and they tacitly "revise the history of [their] beliefs" to match what actually ends up happening. Thus, a person may go from believing (before the fact) that President Nixon would not resign to "having always known" that Nixon would resign. This is not, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) says, mere dishonesty to save face: people genuinely forget that they ever held the ultimately falsified belief. A vast body of popular literature—including many business books—springs from the attempt to find simple causes of success and failure after the fact.

### Chapter 20: The Illusion of Validity

Attempts to predict the future are often beset by the _illusion of validity_, in which a demonstrably useless tool or method is nonetheless assumed to have some special value for forecasting what will happen. People will continue to defer to a particular test, interview, or other metric even when they know rationally that the test is "little better than [a] random guess." Closely related is the _illusion of skill_, in which the technical sophistication of a practice (say, financial analysis of stocks) is assumed to mean that the practice will be effective. Stock traders, Kahneman observes, believe deeply in the efficacy of their own skill even when they fail to outperform (or even significantly underperform) the market as a whole. It is not the experts' fault, he says, that the world is difficult to predict—but it _is_ their responsibility to be honest about the limits of their predictive powers.

## Analysis

The term _illusion of skill_ is in some ways a misnomer. Often, the people who fall prey to such an illusion are indeed highly skilled. The stock traders [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) singles out in Chapter 20, for instance, are probably quite adept at a variety of tasks related to their jobs, such as finding and interpreting the major financial information about a publicly traded firm. The illusion is not that people are skilled when they really aren't, but that their skills are effective in scenarios where they in fact make no difference.

For example, someone may be quite skilled at memorizing facts about the Harry Potter universe. Such a person would likely be effective in winning a themed trivia competition but ineffective at summoning spirits or making broomsticks fly. The difference is that Harry Potter fans have the opportunity to receive consistent and legible feedback about the limited applicability of their knowledge. Nobody gets the experience of _almost_ levitating a broomstick. The stock traders, on the other hand, are exposed to a flood of feedback, making it difficult to separate the effects of skill from those of random chance. There is, in many real-life scenarios, no warning bell to signal when a person has crossed into a domain where their skills will be ineffective.

The distinction applies equally to the _illusion of validity_. The interviewers who assess candidates' fitness for officer training may be highly skilled in many respects. Those skills, such as listening carefully or getting people to speak in a candid and relaxed fashion, are not themselves illusory. Instead, the illusion lies in an overestimate of those skills' effects—in where the skills can be applied, and in how much of a difference they make.
___
#  Part 3, Chapters 21–22 : Overconfidence | Summary

## Summary

### Chapter 21: Intuitions vs. Formulas

Because of the various systematic biases in human judgment, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) proposes that algorithms and formulas can often do a better job than an expert's intuition. However, he acknowledges that there is a widespread "hostility to algorithms" in areas where the outcome may have a moral significance—for instance, in health care. After describing his own experiences in developing a scored survey for assigning soldiers to service branches, Kahneman invites the reader to "do it yourself." He offers some tips for developing a quantitative scoring system to be used in a situation in which intuitive judgment would be the norm.

### Chapter 22: Expert Intuition: When Can We Trust It?

Although he maintains that the public's faith in expert judgment is sometimes misplaced, Kahneman does not rule out the existence of expert intuition in some specialized domains. He tells of a research program undertaken with a colleague, psychologist Gary Klein. Klein is generally seen as an opponent of the heuristics-and-biases view of decision making. In their research, Kahneman and Klein explored the limits of a decision paradigm called _recognition-primed decision_. This model treats expert intuition as the ability to recognize and act on patterns that might not be apparent to a layperson or a novice. Such recognition, Kahneman suggests, is at work when a chess grandmaster glances at a board and can tell immediately who will win the match, or—to use an example from Klein—when a fire captain orders his crew to evacuate just before a building starts to collapse. However, such intuitions can only arise in environments that are "sufficiently regular to be predictable" and can only be developed through long and consistent practice. In fields in which those conditions are lacking, expert intuition is likely to disappoint.

## Analysis

[Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s work with Klein is an example of _adversarial collaboration_, a practice he briefly and somewhat self-effacingly mentions in Chapter 22. In fact, Kahneman champions this form of collaboration, in which someone seeks out and works side by side with the colleagues whom he or she most strongly disagrees with. Klein, as hinted in Chapter 22, is a proponent of a somewhat different model of decision making than the heuristics and biases studied by Kahneman. His preferred theoretical framework, called _naturalistic decision making_ or _NDM_, overlaps in some respects with heuristics and biases but places greater emphasis on the successes of the decision-making process in the face of adverse conditions. Their joint paper, "Conditions for Intuitive Expertise: A Failure to Disagree" (2009), surveys both the commonalities and the differences between the two psychologists' views of decision making under uncertainty.

For Kahneman, adversarial collaboration stands as an alternative to the traditional "critique-reply-rejoinder" process in which social scientists frequently voice their disagreements. This process takes the form of letters to the editor or articles in an academic journal, with one scientist critiquing another's work, the original author replying, and the critic then issuing a rejoinder to the reply. Kahneman's skepticism regarding the merits of this approach comes, at least in part, from the often sarcastic and sometimes outright hostile way in which critique-reply-rejoinder communication is carried out. Coauthoring a paper naturally takes more time—Kahneman and Klein's joint effort took several years—but the result is all but guaranteed to be more substantial and less combative than a series of one-sided polemics. A more thorough explanation of adversarial collaboration and its merits can be found in Kahneman's short autobiography at the Nobel Foundation website.
___
# Part 3, Chapters 23–24 : Overconfidence | Summary

## Summary

### Chapter 23: The Outside View

In this chapter, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) tells of his experiences as part of a team tasked with drafting a textbook on judgment and decision making. The team's internal estimates of the time to complete the project averaged about three years, even though other textbook-writing projects of similar length and complexity had tended to take seven or more years. In fact, the book took nine years to complete—well within statistical expectations but far longer than the team members themselves had predicted. Kahneman takes this anecdote as an illustration of the need to consider the _outside view_—the dispassionate, neutral, statistically informed view—when planning a project. The _inside view_, which is overreliant on the specifics of the situation, is appealing but almost certainly inaccurate. Those who unrealistically favor the more optimistic inside view are committing what Kahneman calls the _planning fallacy_, allowing the detailed nature of their own forecasts to trump the relevant statistical information.

### Chapter 24: The Engine of Capitalism

An "optimistic bias," Kahneman says, is not an altogether bad trait to possess. Optimists tend to live longer, be happier, and be more proactive in solving problems within their control. At the same time, optimism is indeed a type of cognitive bias, and it can be very costly at times. One consequence of optimism in business is the phenomenon of _competition neglect_, in which entrepreneurs assume that their decisions—irrespective of their competitors' actions—are the ultimate determiner of success or failure. Other types of optimism-induced overconfidence are evident in finance, in medicine, and in day-to-day life. As a means of (partly) counteracting optimistic bias, Kahneman passes along a suggestion from Gary Klein, who advocates conducting a _premortem_ of any major plan before putting it into action. The goal of such an exercise is to imagine how the plan might fail despite the best intentions of all involved.

## Analysis

Gary Klein, here credited with the idea of the _premortem_, is mentioned in Chapter 22 as a main scholarly "adversary" of [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s. The two collaborated on a joint work, described in Chapter 22, as a means not so much to resolve as to clarify their differences. Klein's major popular work is _Sources of Power: How People Make Decisions_ (1998). As might be expected from a leading researcher in the field of naturalistic decision making or NDM, _Sources of Power_ emphasizes the successes of the mind's intuitive decision-making process, whereas _Thinking, Fast and Slow_ charts the often-amusing and sometimes disastrous failures. In part, the researchers' different emphases can be chalked up to the different populations they have tended to study. Kahneman, as he acknowledges in _Thinking, Fast and Slow_, has focused on cases where there is often too much "noise" to accurately size up a situation—such as the trading of individual securities on the stock market. Klein's favored subjects are professionals in areas such as firefighting, where expert judgment is demonstrably rewarded and errors can often be identified in real time. In any case, Klein's book makes for an interesting compare and contrast when paired with that of his "adversarial collaborator."

The concept of the _premortem_ has its roots in the more familiar term _postmortem_ (Latin for "after death"). Used as a noun, postmortem is synonymous with "autopsy": a surgical examination of a corpse to try to determine how the person died. The term is often figuratively applied to such metaphorical "deaths" as the failure of a project or the breakdown of a mechanical system. In each case, the principal goal is typically to identify the "cause of death." What component failed in the system? Where did the project plan jump the tracks? The goal of the premortem, as Klein explains in a _Harvard Business Review_ (2007) article on the subject, is to perform this kind of thinking in advance, achieving a kind of "prospective hindsight" about the problems within—and outside of—the planners' control.
___
# Part 4, Chapters 25–26 : Choices | Summary

## Summary

### Chapter 25: Bernoulli's Errors

In Part 4, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) broaches the subject of behavioral economics, an area he credits Tversky with introducing him to. He begins by describing two "species" from behavioral economics: _Econs_, who are perfectly rational and consistent; and _Humans_, who have all the biases and inconsistencies of real human beings. _Behavioral economics_, the subject of the next few chapters, is concerned with refining economic models to address ways in which Humans differ from Econs.

First, however, Kahneman lays the groundwork of decision making in traditional economics. In _expected utility theory_, a person's decisions are assumed to maximize utility, or the benefit derived from the choices available. This includes gambles: if a person prefers apples to bananas, then theoretically, that person will also "prefer a 10% chance to win an apple to a 10% chance to win a banana." Some principles of this theory can be traced back to Swiss mathematician Daniel Bernoulli (1700–82), who observed that people react to relative, not absolute, changes in their wealth. This is just one way in which expected utility—the "psychological value or desirability" of a good—does not perfectly align with monetary value. Despite its brilliance, Kahneman says, Bernoulli's utility theory overlooked a key aspect of subjective value: its dependence on a reference point. It is, he asserts, the change in a status that matters, not the status itself. In particular, people will place greater weight on losses than on gains. Faced with a sure gain or the chance of a greater gain, many will prefer the sure thing, but faced with a sure loss or the chance of a greater loss, most will prefer the gamble.

### Chapter 26: Prospect Theory

In their 1970s research, Kahneman and Amos Tversky attempted to account for the gaps between expected utility theory and the psychology of real-life decision making. They concluded that people in general experience loss aversion, meaning people would rather avoid losses than seek gains. This aversion can even be quantified as a ratio: typically, the two researchers found, people were twice as averse to loss as they were attracted to gain: a 50% chance to win $200 balances out a 50% chance to lose $100. At the same time, sensitivity to loss diminishes as the losses get larger, so that losing $200 is not "twice as bad," psychologically speaking, as losing $100. These insights combine to create a characteristic decision-making pattern: when faced with a win-or-lose gamble, most people will be very risk averse, but when faced with only losing choices, people will risk a greater loss to have a chance of not losing at all. Though he admits it is not a perfect characterization of economic decision making, Kahneman describes prospect theory as a marked improvement over the earlier expected utility model.

## Analysis

Prospect theory, though it includes many useful refinements of utility theory, is not immune from criticism. [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) himself points out some phenomena that are evident to a casual observer, but for which prospect theory, at least in its early form, offered no explanation. Such "blind spots" include disappointment, regret, guilt, and the anticipation of those emotions—all of which play a clear role in economic decision making. A great deal of follow-up research has gone into identifying the many emotional reactions that demonstrably _do_ shape decisions but are not addressed by prospect theory. The emerging field of neuroeconomics promises to employ neuroscience in the same way "traditional" behavioral economics has utilized psychology, developing a still more sophisticated model of human decision making under uncertainty.

A related issue for prospect theory might be described as "domain creep." Initially, Kahneman and Tversky intended prospect theory as an account of individual economic decision making under a fairly restricted set of circumstances. Like the rational-agent theory to which it responded, prospect theory was never meant to assert a uniform set of rules that could be applied to all human judgments and decisions. It would be misguided, even absurd, to apply the tenets of rational-agent decision theory to human beings in general; the range of human behaviors is simply too broad to support the assertion that people are logical and single-mindedly selfish 100% of the time. Likewise, it would be a mistake to treat prospect theory as a skeleton key to the human psyche when its authors originally proposed it within the narrow scope of wins, losses, and gambles. Still, the appeal of prospect theory is such that it has been used to explain all sorts of not-quite-economic phenomena, including the resolutions of political crises and the results of elections. These are, to use a pharmaceutical analogy, off-label uses of the theory, with little grounding in Kahneman and Tversky's initial experiments or those of their successors.

The popularity of Kahneman's ideas—not only prospect theory, but also the heuristics-and-biases model in its broadest sense—seems to ensure that they will be overgeneralized well beyond their true explanatory value. Consequently, Kahneman continues to fight an uphill battle to correct misconceptions about when and how his theories apply to real-world phenomena.
___
# Part 4, Chapters 27–29 : Choices | Summary

## Summary

### Chapter 27: The Endowment Effect

Further complicating the picture is the _endowment effect_: people tend to overvalue what they have, once they have it. A person who is equally happy to receive either a raise or some added vacation time will, once they receive the raise, often be unwilling to trade it back for the vacation time. This effect occurs at all scales, from coffee mugs and chocolate bars to rare antiques and vintage wines. The possessor of a good—provided it is "held for use" and not merely a commodity or currency—will overrate the value of the good when considering a sale or exchange. [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) cites several experiments in which this effect is established and investigated.

### Chapter 28: Bad Events

Yet another twist comes from what Kahneman calls _negativity dominance_: the tendency to see the one angry face in a crowd of smiles, but not vice versa. There is, Kahneman suggests, an evolutionary reason to give threats greater priority than opportunities, since an opportunity means nothing if one does not survive to enjoy it. Loss aversion, as discussed in Chapter 26, is one instance of negativity dominance; another is the aversion to falling short of a goal. Hence, Kahneman argues, the better performance of golfers when putting for par than when putting for a birdie. Negativity dominance has frustrating consequences for negotiations, whether they be economic or political: both parties feel that what they are giving up is more valuable than what they are getting in return.

### Chapter 29: The Fourfold Pattern

A final piece of the prospect theory puzzle comes in the form of two complementary effects: the _possibility effect_ and the _certainty effect_. The former alludes to the tendency to overweight the mere possibility of an unlikely event, as seen in lottery ticket buyers around the world. The latter alludes to the premium paid for the last few percentage points between an almost-sure thing and a true certainty. Both effects pose a further challenge to expected utility theory, as Kahneman proceeds to show by recounting a famous economic puzzle posed by Maurice Allais (the "Allais paradox"). Kahneman now returns to the basics of prospect theory (see also Chapter 26) and describes the "fourfold pattern" of decision behavior in the face of gains and losses. When people have a high probability of a gain, they fear disappointment and are generally willing to accept a smaller, sure-thing payment rather than gamble, but when they face a high probability of a loss, they are typically willing to take their chances. The pattern is reversed for low-probability events: a small chance of a large gain is preferred to a sure-thing settlement, while a smaller sure loss is preferred to a small chance of a large loss. Together, these four effects illustrate the interaction of loss aversion and diminishing sensitivity (both seen in Chapter 26) with the certainty and possibility effects. The chapter closes with some illustrations drawn from the world of litigation.

## Analysis

To understand the fourfold pattern and its relationship to prospect theory, it may help to review an example in more detail. Four different people—Alice, Bob, Cathy, and Dylan—are each offered a gamble to consider:  

- **Alice** is asked to choose between a 95% chance of winning $5,000 and a guaranteed payment of $4,500.
- **Bob** must decide between a 95% chance of losing $5,000 and a guaranteed loss of $4,500.
- **Cathy** faces a choice between a 5% chance of winning $5,000 and a guaranteed payment of $500.
- **Dylan**'s two options are a 5% chance to $5,000 or a guaranteed loss of $500.

Here is how prospect theory would predict each of the four subjects to respond, along with a brief explanation of the psychological principles at work:

- **Alice** will likely _accept_ the $4,500 sure-thing payment, even though the expected value of the gamble is higher ($4,750 to be exact). The "lost" expected value can be chalked up to the _certainty effect_: Alice would _much_ rather be sure of winning than "almost sure," and she is willing to "pay" $250 in expected winnings to eliminate the uncertainty.
- **Bob** will likely _reject_ the $4,500 loss—again, even though the expected value of the gamble is a worse loss ($4,750). _Loss aversion_, combined with _diminishing sensitivity_ to loss, account for Bob's likely tendency to overestimate the slim chance of losing nothing in the gamble.
- **Cathy** will likely _reject_ the $500 sure-thing payment and prefer the gamble, even though the gamble has an expected value of only $250 ($5,000 × 5%). If she indeed chooses the gamble, she is illustrating the _possibility effect_ by giving disproportionately large weight to a small chance of winning. This is the scenario [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) invokes to explain "why lotteries are popular."
- **Dylan** will likely _accept_ the $500 sure-thing payment, even though the gamble brings an expected loss of only $250. The _possibility effect_ is at play here as well, making the slight possibility of losing $5,000 loom larger than it would to an Econ. Kahneman would liken Dylan's situation to a person buying insurance, paying a premium up front not to have to worry about "an unlikely disaster."

Together, as the above four gambles show, loss aversion and diminishing sensitivity (Chapter 26), plus the possibility and certainty effects (Chapter 29), account for some pretty substantial differences between the Humans of prospect theory and the Econs of rational-agent theory. An Econ, or a sufficiently disciplined and well-capitalized trader, would likely make the exact opposite choice in each of the four scenarios, trusting the expected value to reward such decisions in the long haul. The Human/Econ contrast seen here thus helps to explain how large classes of economic activity—the casino business and the insurance market, for instance—arise seemingly in spite of purely rational principles. As such, the fourfold pattern captures some key benefits of extending the rational-agent model to include at least a few simply stated behavioral principles.
___
#  Part 4, Chapters 30–32 : Choices | Summary

## Summary

### Chapter 30: Rare Events

In general, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) says, people tend to "overestimate the probabilities of unlikely events" and "overweight unlikely events in their decisions." Several psychological mechanisms contribute to this tendency, including the availability heuristic and the tendency to prefer cognitive ease to cognitive strain. The broadest explanation, however, is that the _vividness_ of rare events tends to give them a disproportionate share in decision making. The mind's focus on vivid outcomes can lead to _denominator neglect_: in the phrase "1 out of 100,000 children will be disabled as a result of the vaccine," the emphasis tends to fall on the "1" rather than the "100,000." The more concretely the probability is represented (e.g., "1 child in 100,000" rather than "0.001% of children"), the more pronounced such overemphasis will be. "When it comes to rare probabilities," Kahneman soberly concludes, "our mind is not designed to get things quite right."

### Chapter 31: Risk Policies

The discussion now turns to ways in which risk can be approached more systematically. _Narrow framing_ of a problem—approaching it as a set of unrelated one-off choices—can lead to suboptimal outcomes in many cases. _Broad framing_ is required to see how choices interact, to get a "big picture" that transcends individual losses and gains. To illustrate the distinction, Kahneman asks why someone might reject a single, 50/50, "win $200 or lose $100" bet but accept the option of making 100 such bets. If a broad frame is adopted, he says, it is obvious that the 100 bets are massively favorable to the gambler. Yet if each gamble is viewed as a distinct, isolated event, loss aversion kicks in: the thought of losing $100 is often more painful than the thought of winning $200. In a rare moment of direct advice to the reader, Kahneman points out that life itself contains many "small favorable gambles." Accepting them as a matter of course, he adds, will lead to a better result than rejecting each one individually out of loss aversion. This is an example of a _risk policy_—a commitment to handle a particular risk the same way every time, in order to come out ahead in the long run.

### Chapter 32: Keeping Score

Another quirk of human reasoning is _mental accounting_: the tendency to reckon up money and other resources in separate "accounts" rather than as a lump sum. Such mental accounts are, Kahneman observes, "a form of narrow framing," but they help in making sense of the world. One adverse consequence of mental accounting, however, is that it opens the door to the _sunk-cost fallacy_. People who spend money for a specific purpose—to see a ball game, for instance—will often "throw good money after bad" if an added expense arises in connection with the event. Stockholders will sell winners rather than losers so as to close out their position as a gain rather than a loss. Regret and the anticipation of regret, along with feelings of moral responsibility, further complicate the effort to "keep score" of finite resources.

## Analysis

In Chapter 32, [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) introduces the _sunk-cost fallacy_, a potentially confusing consequence of humankind's mental-accounting tendencies. The basic idea is that a nonrefundable expense is a sunk cost: if there is no way to recover it, then the cost should have no effect on subsequent decisions, at least from the viewpoint of economic rationality. The _fallacy_—the failure of logical reasoning—lies in treating sunk costs as an investment to be recouped or salvaged, not merely as a cost to be accepted. The classic illustrations, including those presented by Kahneman, involve a monetary cost, but sunk costs and their follow-on expenses can also take the form of effort or inconvenience. If one pays for an all-you-can-eat buffet only to find the food is no good, the price of the meal is a sunk cost, and there is nothing to be gained by stuffing oneself to "get one's money's worth." A person who stays in an unfulfilling relationship on the grounds that they have "invested" time in the relationship is, likewise, succumbing to sunk-cost thinking.

Business, psychology, and general-interest periodicals have published numerous articles on sunk cost phenomena, and many of them—such as journalist Jamie Ducharme's _Time_ article "The Sunk Cost Fallacy Is Ruining Your Decisions. Here's How" (2018)—have a self-help tinge. "If you've ever let unworn clothes clutter your closet just because they were expensive," explains Ducharme, "or followed through on plans you were dreading because you already bought tickets, you're familiar with the sunk cost fallacy." A related phenomenon, though with a probabilistic twist, is the _gambler's fallacy_: the belief that one is "due for a win" after sustaining or witnessing many losses. The opposite of the sunk-cost fallacy—the tendency to consider each new expenditure on its own merits—has also been given a name: the _bygones principle_, as in "let bygones be bygones."

To complicate matters further, it is not _always_ logically incorrect to keep past costs in mind when considering new ones. For this reason, researcher Christopher Olivola (quoted in Ducharme's article) prefers the term "sunk cost _effect_." "That effect becomes a fallacy," Olivola argues, "if it's pushing you to do things that are making you unhappy or worse off." In some situations, social factors may make it reasonable to honor sunk costs even though there is no monetary incentive to do so. Ryan Doody, whose research involves the intersection of philosophy and economic decision making, argues as much in his 2017 paper "The Sunk Cost 'Fallacy' Is Not a Fallacy." Despite the provocative title, Doody's main contention is a rather mild one: sometimes, it is worthwhile to honor a sunk cost simply so that one can later offer a flattering account of one's actions. In many situations, there is value—even economic value—in being known for perseverance rather than as a quitter.
# Part 4, Chapters 33–34 : Choices | Summary

## Summary

### Chapter 33: Reversals

A _preference reversal_ arises when people make one choice when presented simultaneously with a pair of options ("B over A"), but another choice if the two options are considered in isolation ("A over B"). [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) cites such reversals as another feature of human economic reasoning not adequately explained by the rational-agent model. The solution to the apparent paradox, he says, is quite simple: people's assessments of value are based on the context in which the question is asked. A six-year-old boy who is 5 feet tall is "tall" relative to other boys his age, and a 16-year-old boy who is 5 feet 1 inches tall is "short" for his age, but in a direct comparison it is obvious that the latter boy is taller. The _joint evaluation_ of the two boys produces a different type of comparison than the _single evaluation_ of each boy relative to his age group. In economic decisions, too, this kind of context-based reversal can be observed: a fine that is high by one agency's standards may be a pittance by the standards of another agency.

### Chapter 34: Frames and Reality

Kahneman explores the concept of _framing effects_, in which the mere wording of a decision problem substantially changes people's preferences. He reports that people are much more likely to recommend a procedure with a 90% survival rate than one with 10% mortality. They will endorse a program that "saves 200 lives out of 600" but reject one that results in 400 deaths. These effects are important, Kahneman says, because often there _is_ no underlying preference: the frame itself determines what moral intuitions people bring to bear. "An important choice," he remarks, "is controlled by an utterly inconsequential feature of the situation."

## Analysis

These two chapters supply some of [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s heaviest ammunition against the _rational-agent theory_, the view of human behavior that asserts an underlying rationality and consistency to human behavior. Though Kahneman does not deny the effectiveness of this view as an approximation (e.g., in economics), he finds too many departures from rationality in day-to-day decision making to endorse the rational-agent model. Importantly, this is not the same as saying that humans are irrational, or that their decision-making rules are totally divorced from logic. Indeed, Kahneman is careful not to use the word _irrational_, which he views as an excessive and potentially misleading descriptor of the biases in human judgment. Still, there is something not strictly rational about a system of decision making that can be swayed simply by changing the wording of a question, or by considering options in isolation rather than side by side.

The preference reversals described in Chapter 33 can be found well outside the laboratory. In marketing research, a similar phenomenon is well known as the _decoy effect_, wherein a high price comes to seem lower if a "decoy" price is provided for comparison. The classic example is movie theater popcorn: if a small serving of popcorn sells for $4 and a large serving goes for $8, people will generally choose the smaller serving. Introducing a medium-sized serving for $7 drastically changes consumers' preferences: when the large popcorn is framed as a $1 "upgrade" over the medium, the extra expenditure suddenly seems worthwhile.

From the viewpoint of joint and single evaluations, this may seem like an unusual case, since the small and large servings are being evaluated jointly in both cases. The peculiarities of the decoy effect can be traced to several principles already presented in _Thinking, Fast and Slow_. Consistent with the laziness of System 2, people faced with three options will not perform a separate joint evaluation for each pair of options. Rather, they will tend to _anchor_ on the medium-sized serving and compare it with the adjacent options. From there, they are likely to conclude that going from medium to large gives them the most "bang for their buck."
___
# Part 5, Chapters 35–38 : Two Selves | Summary

## Summary

### Chapter 35: Two Selves

This chapter introduces the two characters who will star in Part 5: the _experiencing self_ and the _remembering self_. The experiencing self is present in the moment, undergoing pleasure or pain. The remembering self recalls experiences after the fact and makes decisions based on those memories. Memories themselves are prone to persistent biases, such as an overemphasis on the best (or worst) moments of an experience and, more troublingly, a neglect of duration. "We want pain to be brief and pleasure to last," [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) observes, but _duration neglect_ ensures the remembering self will not make choices accordingly.

### Chapter 36: Life as a Story

Kahneman observes that stories are often defined by their endings. This is as true, he maintains, of operas as it is of a person's lifetime. Citing research on perceived quality of life, Kahneman shows how "peaks and ends matter but duration does not." That is, a person who lives 60 extremely happy years and then dies suddenly is consistently judged to have had a better life than a person who lives a further five "slightly happy" years. When planning a vacation, likewise, the remembering self is in the driver's seat: many people would not bother to make a trip from which they could not bring back happy memories.

### Chapter 37: Experienced Well-Being

Kahneman surveys some standard approaches to measuring happiness, methods that Kahneman regards as overreliant on the remembering self rather than the experiencing self. He presents his own efforts to measure an individual's _U-index_, a term he and his colleagues used to denote the percentage of time spent in an unpleasant state. Such an index, he suggests, can also be applied in aggregate as a loose measure of a population's well-being. Things that profoundly affect well-being in the moment, Kahneman reports, may have a smaller or even opposite effect on overall life satisfaction—and vice versa.

### Chapter 38: Thinking About Life

Finally, Kahneman digs deeper into the issues inherent in any attempt to measure life satisfaction. He reviews some experiments in which life satisfaction measures are shifted considerably by trivial occurrences (e.g., finding a dime) as well as by major events (e.g., a recent marriage). Collectively, Kahneman describes these results as evidence of the _focusing illusion_, in which people give exaggerated importance to whatever they are thinking about at the moment. In considering the purchase of a new car, for example, people routinely overestimate how big a role the car will play in their overall happiness. This illusion compromises predictions about what will bring future happiness.

## Analysis

These chapters lay out a partitioning of the self that is different from, but complementary to, the System 1/System 2 dichotomy used in earlier parts of the book. Both the _experiencing self_ and the _remembering self_ are prone to the kinds of errors [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) associates with System 1 (the "fast" system). In fact, _duration neglect_ and the _peak-end rule_ (both cited in Chapter 35) contribute to the perpetuation of a fundamental System 1 pattern. Memory, like the mind in general, prefers to see the world in terms of averages, rather than in terms of sums.

The "overall unpleasantness" of an experience, if such a thing can be said to exist, would surely depend to some extent on the episode's duration. However, System 1 does not think in such terms. Instead it rates a tense and awkward 15-minute meeting as "more unpleasant" than a mildly boring two-hour seminar. Consequently, the remembering self takes much more care to avoid short, highly unpleasant experiences than to avoid long, mildly unpleasant ones. Thrill-seeking behavior arises from the same cognitive quirks: under duration neglect, a half-hour of euphoria counts for more than a moderately pleasant weekend at the beach.

Kahneman's discussions of life satisfaction and the challenge of measuring it (Chapters 37–38) are rooted in the fundamental heuristics and biases presented throughout _Thinking, Fast and Slow_. One example is the _availability heuristic_, which Kahneman touches on (though not by name) in Chapter 38 as a source of difficulty in coming up with a "score" for one's happiness with life. First introduced in Chapter 12, the availability heuristic is the tendency to substitute availability—the ease with which one comes up with examples—for frequency. A person using the availability heuristic to think about life satisfaction will resort to "available" images and events from recent experience rather than making a systematic walkthrough of job, career, relationships, and so forth.

Also relevant here is the representativeness heuristic, in which the frequency of an event is judged by how typical or "representative" the event is. Duration neglect and the peak-end rule are, in a sense, a pattern of representativeness-based thinking in the specific domain of memory. The peak-end rule ensures that the highlights (or lowlights) of an experience come to represent the whole thing, and duration neglect means that a preponderance of equally "representative" moments are simply ignored.
___
# Conclusions | Summary

## Summary

_Thinking, Fast and Slow_ concludes with a review of the three main dichotomies presented in the book: System 1 versus System 2; Humans versus Econs; and the remembering self versus the experiencing self.

The "two selves" are treated first, since they appeared most recently. Humans rely on their imperfect memories in making decisions, so the remembering self is at the helm whenever a choice is made about one's future happiness. This is true even though the experiencing self—the self that lives out the moment-by-moment results of the decision—may experience pain or pleasure that differ systematically from those predicted by the remembering self. What makes for a pleasant experience is not always what makes for a happy memory, and people often make suboptimal choices because they forget important features of their past experiences. [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/) urges psychologists and policymakers to adopt definitions of well-being that integrate a person's experiences, memories, and goals.

_Econs_ (a term borrowed from Richard Thaler and Cass Sunstein) are the fictitious, perfectly logical beings who follow the simple rules of rational-agent economics. They do not make mistakes in reasoning or succumb to cognitive biases. _Humans_, as contrasted with Econs, are the logically fallible beings who populate the real world and participate in its economy. Libertarianism, Kahneman suggests, is predicated on the assumption that people are Econs, always rationally following their own self-interest. Behavioral economists, like Kahneman and his colleagues, are skeptical about the rationality of humans and favor policy programs designed to help people make better choices. Kahneman cites several examples of _libertarian paternalism_, an approach in which people are steered toward—but not coerced into—responsible choices about their health and finances.

Finally, Kahneman returns to Systems 1 and 2, a pair introduced in the earliest chapters. These are Kahneman's terms for the two different systems the human mind uses to solve problems and produce judgments. System 1, he reminds the reader, is the "fast" system of largely unconscious cognitive patterns used to find quick, approximate solutions. System 2 is the deliberate, "slow" system that sometimes serves to correct the errors of intuition and bias. Recognizing one's cognitive errors is difficult; somewhat easier is the task of recognizing errors in others' thinking. In either case, Kahneman suggests, the vocabulary offered in _Thinking, Fast and Slow_ makes the effort more fruitful.

## Analysis

_Libertarian paternalism_, described enthusiastically by [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/), is the principle of making certain behaviors—saving money, for instance, or eating more vegetables—"opt-out" rather than "opt-in." On a practical level, this principle is often implemented by means of so-called nudges: small reminders or interventions that encourage (but do not require) desirable behavior, or that discourage (but do not forbid) undesirable behavior. The two scholars best known for their work on the subject are legal scholar Cass Sunstein and economist Richard Thaler, both of whom Kahneman cites not only in [Conclusions](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/conclusions-summary/), but also throughout the pages of _Thinking, Fast and Slow_. In their own book, _Nudge: Improving Decisions About Health, Wealth, and Happiness_ (2009), Thaler and Sunstein describe interventions that nudge people toward greater retirement savings, more sustainable spending habits, and healthier cafeteria choices.

As Kahneman and his colleagues predicted, the concept of the nudge has found widespread support among both researchers and policymakers in the years since Thaler and Sunstein's work was published. In an article titled "Good for Some, Bad for Others: The Welfare Effects of Nudges" (2019), behavioral economist Linda Thunström writes that "it is easy to see the appeal of using nudges to change people's behavior ... They are often low in implementation cost, high in public support, and may result in substantial behavioral change." Nonetheless, Thunström warns, even well-intended nudges can fail or, worse yet, backfire. For example, diners who do not want to fret over their food choices may simply ignore a new menu-labeling system. A secondary yet significant concern is that those being nudged will, despite Sunstein and Thaler's emphasis on noncoerciveness, come to feel that they are being pushed more than nudged.

Ultimately, a successful nudge intervention requires the cooperation, whether conscious or unconscious, of the individuals whose lives it is intended to improve. Some such interventions—such as placing the salad bar front and center in a cafeteria—may not even feel like nudges from the end user's point of view. However, others are hard to ignore, and such messages may be avoided or filtered out by those who do not appreciate continual reminders "for their own good." "I am convinced that I am better off from any default nudge that encourages savings," notes Thunström, "but stubbornly ... avoid the nudge that informs me about the calorie content of my afternoon latte—I want to wholeheartedly enjoy my latte experience."
___
#  Quotes

1.

> We can be blind to the obvious, and we are also blind to our blindness. 

Narrator, [Part 1, Chapter 1](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-1-introduction-chapter-2-summary/)

_Thinking, Fast and Slow_ is, in a sense, a tour of the blind spots of human cognition. Kahneman shows how the intuitions that guide everyday thinking can fail—sometimes in mundane ways, but sometimes quite spectacularly. A recurring theme in the book is the limits of human intuition when confronted with rare or unexpected phenomena. These blind spots are significant because modern life often requires judging probabilities and assessing statistical trends—tasks in which intuition can be more hindrance than help.

2.

> In the economy of action, effort is a cost ... Laziness is built deep into our nature. 

Narrator, [Part 1, Chapter 2](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-1-introduction-chapter-2-summary/)

When Kahneman discusses the "laziness" of the human mind, he is not preaching or rendering a moral judgment. From his point of view, as argued throughout the book, "laziness" is an adaptive trait that saves unnecessary effort and preserves scarce resources. In many day-to-day scenarios, the brain's reluctance to engage in effortful, focused thinking could just as easily be termed "efficient" as "lazy." The trouble arises not from the general avoidance of cognitive effort, but from a failure to recognize when extra mental effort would make a significant difference.

3.

> If you care about being thought credible and intelligent, do not use complex language where simpler language will do. 

Narrator, [Part 1, Chapter 5](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-1-chapters-3-5-summary/)

This is just one of several applications of the concept of _cognitive ease_. If a statement is memorable and easy to comprehend, Kahneman says, it is likely to be seen as true and insightful as well. Messages that create less _cognitive strain_ are received more favorably and scrutinized less carefully than cognitively demanding messages with the same literal meaning.

4.

> Jumping to conclusions is efficient if the conclusions are likely to be correct. 

Narrator, [Part 1, Chapter 7](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-1-chapters-6-7-summary/)

Kahneman's discussion of "jumping to conclusions" follows his earlier description of cognitive "laziness." He shows how the drive to conserve cognitive effort can be beneficial, provided the stakes are low enough. The cost of jumping to conclusions becomes truly significant only when there is a substantial penalty for making a mistake—for example, when diagnosing an illness or when buying a house.

5.

> Plans are best-case scenarios. 

Narrator, [Part 2, Chapter 11](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-2-chapters-10-12-summary/)

In Chapter 11 Kahneman briefly touches on the ways in which plans can go awry. In Chapter 23 he provides an illustration of the ideas from his own multiyear involvement in a textbook-writing venture that went far past its original deadline. Kahneman concludes that the people planning any enterprise must try to get the _outside view_ on their project. For example, they would ask, "How long do most books this length take to write?" They must try to treat their effort as if it were part of a statistical class and not a one-off event.

6.

> People who ... assess probability are not stumped ... they do not ... judge ... as statisticians and philosophers. 

Narrator, [Part 2, Chapter 14](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-2-chapters-13-15-summary/)

The first three parts of _Thinking, Fast and Slow_ are concerned, to varying degrees, with the mismatch between two kinds of probability. In the strict mathematical sense, probability is a precisely but narrowly defined concept, more applicable to statistical phenomena than to one-off events. However, most people use probabilistic terms informally to describe their relative degree of belief that something will happen ("I'm 90% sure he'll be late"). Kahneman does not quibble with this usage. However, he does point out some of the difficulties that can arise when the intuitive thinking of System 1 is treated _as if_ it were genuine probabilistic reasoning.

7.

> We are statistically punished for being nice and rewarded for being nasty. 

Narrator, [Part 2, Chapter 17](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-2-chapters-16-18-summary/)

This observation comes in the middle of a larger discussion of _regression to the mean_: the statistical tendency for extreme events to be followed by more "average" ones. It's natural to be nice to people who are being kind, helpful, or competent, and to be rude to people who are being uncooperative or hostile. However, in general people's behavior also regresses to the mean. An athlete who has an exceptionally good game and is praised by the coach will, for purely statistical reasons, likely do worse at the next game—effectively "punishing" the coach for being nice. An athlete who has an exceptionally bad game and is berated by the coach will likely do better at the next game, effectively "rewarding" the coach for losing his temper.

8.

> In the presence of randomness, regular patterns can only be mirages. 

Narrator, [Part 3, Chapter 19](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-3-chapters-19-20-summary/)

Kahneman is not, as a rule, very optimistic about the power of experts to predict the future in politics, finance, or other domains where experts are frequently sought. His reservations partly stem from a recognition that people are very good at identifying patterns in randomness—where, by definition, no patterns actually exist. This tendency is particularly pronounced in hindsight, creating an "I knew it all along" effect that is both compelling and illusory. Commentators who "knew all along" what would happen last year often come to believe that they know what will happen next year, too.

9.

> Intuition is nothing more and nothing less than recognition. 

Narrator, [Part 3, Chapter 22](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-3-chapters-21-22-summary/)

Where genuine expert intuition does exist, Kahneman follows and quotes the explanation advanced decades earlier by Herbert Simon, a American Nobel laureate recognized for his contributions to psychology, mathematics, statistics, and economics, and his pioneering research into artificial intelligence. Simon explained expert intuition as the ability, acquired through long experience, to recognize patterns too complicated or subtle to be distinguished by nonexperts. Kahneman continues this line of thinking by suggesting that such experience can only be accrued and made sense of in some fields of human endeavor. True expert intuition can only exist in those particular circumstances.

10.

> The agent of economic theory is rational, selfish, and his tastes do not change. 

Narrator, [Part 4, Chapter 25](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-4-chapters-25-26-summary/)

Kahneman quotes this concise statement of the underlying principle of _rational-agent economics_ written by Swiss economist Bruno Frey. Much of Kahneman's career has been devoted to charting the ways in which humans are not so rational or consistent after all. His work on prospect theory, described in this and the following chapters, is largely an effort to create a model of economic decision making that acknowledges deviations from rationality.

11.

> Threats are privileged above opportunities, as they should be. 

Narrator, [Part 4, Chapter 28](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-4-chapters-27-29-summary/)

Over the years, Kahneman's work has frequently been criticized for its emphasis on the biases of human cognition. Kahneman argues that this criticism is missing the point: most of the time, heuristics of judgment work well. Cognitive biases, such as the aversion to loss described in this quotation, are noteworthy because they arise within a generally successful system and often serve a purpose in their own right.

12.

> Highly unlikely events are either ignored or overweighted. 

Narrator, [Part 4, Chapter 30](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-4-chapters-30-32-summary/)

Human cognition is at odds with the assumption of economic rationality in many respects. One noteworthy pattern is the tendency to misestimate—usually, to overestimate—the likelihood or significance of rare events. Kahneman offers several complementary explanations for this cognitive trait, which appears to arise from the ease and vividness with which rare outcomes are imagined.

13.

> The sunk-cost fallacy keeps people for too long in poor jobs, unhappy marriages, and unpromising research projects. 

Narrator, [Part 4, Chapter 32](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-4-chapters-30-32-summary/)

Toward the end of Part 4, Kahneman introduces the notion of _mental accounting_: the practice of treating resources as if they are earmarked for different areas of life. This process helps to keep people sane and organized, but it can also produce some perverse results when combined with loss aversion. One such result is the _sunk-cost fallacy_, the choice to continue to invest in a failing venture, even when there are better options available, rather than declaring defeat and cutting one's losses.

14.

> What we learn from the past is to maximize the qualities of our future memories, not ... our future experience. 

Narrator, [Part 5, Chapter 35](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-5-chapters-35-38-summary/)

Kahneman proposes a distinction between the _experiencing self_ and the _remembering self_. Because memories, not direct experiences, serve as the basis for predictions about the future, the _remembering self_ is in charge of making decisions. Yet memories are prone to their own systematic biases, and a fondly remembered experience may not have been enjoyable in the moment.

15.

> The easiest way to increase happiness is to control your use of time. 

Narrator, [Part 5, Chapter 37](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/part-5-chapters-35-38-summary/)

In an attempt to get away from the "tyranny of the remembering self" referred to above, Kahneman suggests looking at how people spend their time. Activities that bring pleasure and fulfillment in the moment, he suggests, are worth pursuing even if they do not make for exciting memories.
___
# Appendices

The appendices to _Thinking, Fast and Slow_ include two of [Kahneman](https://www.coursehero.com/lit/Thinking-Fast-and-Slow/author/)'s original scholarly publications, both coauthored with his friend and collaborator, psychologist Amos Tversky. Appendix A presents "Judgment Under Uncertainty: Heuristics and Biases," published in 1974. Appendix B presents "Choices, Values, and Frames," published in 1984.

## Judgment Under Uncertainty: Heuristics and Biases

This classic paper is considered the foundational work of heuristics-and-biases psychology. Tversky and Kahneman outline three main cognitive heuristics, and then describe some of the cognitive biases that these heuristics introduce.

First is the _representativeness heuristic_, the tendency to evaluate the probability of an event according to how typical or "representative" it is of a larger category. The authors give the example of a "shy and withdrawn" man who is judged more likely to be a librarian than a farmer. The heuristic is flawed because it does not account for base rates: if there are 20 times as many farmers as librarians in the world, there are almost surely more shy farmers than shy librarians. A reliance on representativeness also leads people to ignore the effects of sample size, even though small samples are likelier to stray from the statistical norm than large samples are. Even when faced with inherently unpredictable events, people will use representativeness to come up with a numerical prediction. The representativeness heuristic is so persuasive to those who employ it that it creates an _illusion of validity_, in which even irrelevant data are assumed to provide a basis for prediction. A related issue is the inability to recognize _regression to the_ _mean_, a statistical concept that Kahneman explains more fully in Chapter 17 of the main text.

Next is the _availability heuristic_. People employ this heuristic when they estimate the frequency of a phenomenon based on the ease with which examples come to mind. There are several inherent biases in using "cognitive availability" as a measure of frequency. The salience and recency of an event, such as a fire a person reads about in the newspaper, can lead the person to overestimate the frequency of similar events. Moreover, some categories—for instance, "words whose third letter is R"—are inherently more difficult to consider than others—"words beginning with R"—and this warps any estimate of frequency.

The third heuristic presented is called _adjustment and anchoring_. This consists of "starting from an initial value that is adjusted to yield the final answer." The bias in this method arises because the anchor value is typically given excess weight, and the adjustments are typically insufficient. The anchor-and-adjust process poses particular problems when an estimate of compound probabilities ("A, B, and C"; "neither A, B, nor C") is required. Estimates of conjunctive ("both/and") probabilities tend to run too high, while estimates of disjunctive probabilities ("neither/nor") tend to run too low. The authors cite examples of such biases from finance, games of chance, and elementary mathematics.

In their conclusion, Tversky and Kahneman anticipate potential objections to their findings. The biases they have uncovered, they assert, are not the result of wishful thinking on the part of the experimental subjects, and they affect even "statistically sophisticated" participants. The authors emphasize that cognitive heuristics are useful in daily life but contend that more must be learned about the ways in which these heuristics can go astray.

## Choices, Values, and Frames

In the second article, slightly less well known than the first, Kahneman and Tversky describe some aspects of human decision-making that complicate the traditional economic picture of rational agents making consistent, self-interested choices. In the evaluation of risk, the authors say, people do not merely weigh the expected value of a gamble. Rather, they are consistently more averse to losses than they are attracted to gains, so that a 50/50 "win $100, lose $100" gamble is consistently avoided. They also find that people experience _diminishing sensitivity_ to both gains and losses. This means that a $500 loss is experienced as less than "twice as bad" as a $250 loss.

Kahneman and Tversky proceed to challenge the principles of invariance and dominance, two fundamental maxims of mainstream decision theory. According to the principle of dominance, people will always pick the more valuable of two choices—preferring, for example, a 50% chance to win $900 to a 50% chance to win $800. According to the principle of invariance, "the preference order between prospects should not depend on the manner in which they are described." The authors show that, contrary to both principles, people are routinely swayed by the mere wording of a problem. Combating such _framing effects_, they concede, is an uphill battle.

In subsequent sections, Kahneman and Tversky show how people routinely overweigh small chances (the _possibility effect_) and underweigh very high ones (the _certainty effect_). This undermines the principle of invariance, which would predict that the difference between a 20% and a 25% chance is the same as the difference between 0% and 5% or 95% and 100%. Citing the work of American economist Richard Thaler (whose research appears frequently in _Thinking, Fast and Slow_), Kahneman and Tversky contemplate how these different psychological effects could be used for policy purposes or exploited for commercial gain.

The final sections of the paper discuss two other cognitive quirks connected to prospects of gain and loss. First is the _sunk-cost fallacy_, in which a person refuses to accept a loss dispassionately. The person ends up "throwing good money after bad"—continuing to invest resources in a failing project, even when there are better options available. Second is the related _endowment effect_, in which people set a premium on assets simply because they possess them. Where this effect is prevalent, a "substantial discrepanc[y] between buying and selling prices" is the result. Both of these phenomena are consequences of _mental accounting_, a concept studied extensively by Richard Thaler and discussed by Kahneman in Part 4 of _Thinking, Fast and Slow_.
___
# Glossary

**availability heuristic:** tendency to estimate the frequency of an object or event by its availability—the ease with which examples come to mind

**base rate:** rate at which a characteristic of interest occurs in the general population

**bias:** attitude or judgment that is unreasoned or not thoroughly considered

**duration neglect:** tendency to discount the duration of an experience in assessing how much pain or pleasure it caused

**Econ:** economic agent who behaves in a perfectly logical, rational, and consistent manner

**experiencing self:** self as it experiences the pain or pleasure of moment-to-moment living

**focusing illusion:** tendency to overrate the importance of whatever one is immediately thinking about

**framing effects:** psychological or behavioral changes introduced by changing how a problem is framed (e.g., its wording) without altering the substance of the decision to be made

**heuristic:** cognitive "shortcut" used to obtain a quick, approximate answer

**hindsight bias:** tendency to overestimate the predictability, or downplay the surprise, of past events

**Human:** economic agent who is prone to errors of reasoning and inconsistency of preferences

**joint evaluation:** experimental setup in which each participant is asked to consider a pair of conditions or scenarios

**miswanting:** making decisions based on an inaccurate prediction of one's future emotions

**regression to the mean:** tendency for a statistical outlier, such as an exceptionally good or bad performance in a sporting match, to be followed by a less extreme event

**remembering self:** self as it takes stock of past experiences and makes decisions concerning the future

**representativeness heuristic:** tendency to estimate probabilities by considering how well an object or event represents the class to which it belongs

**single evaluation:** experimental setup in which each participant is presented with only one of a pair (or group) of scenarios or conditions

**System 1:** set of mental processes by which quick, approximate judgments are formed

**System 2:** set of mental processes by which slow, deliberate reasoning is carried out

**WYSIATI:** "What You See Is All There Is" describes System 1's attempt to construct a cogent story from information readily at hand, without looking for additional information
___
#  Biography

### Early Years

Daniel Kahneman was born in Tel Aviv, in present-day Israel, on March 5, 1934, while his mother was visiting family. His parents were Lithuanian Jewish immigrants to France, and Kahneman lived in Paris during the early years of his life. As World War II (1939–45) commenced, the Nazi invasion of France began in 1940, and Paris was occupied by the German military in June of that year. Kahneman's father, chief of research at a chemical firm, was briefly held in the detention camp at Drancy. He was released, and the Kahneman family sought refuge first in the south of France, and then later in the interior. In 1944 Kahneman's father died of complications from diabetes, just weeks before the D-Day invasion that ultimately liberated France.

After the war, Kahneman, his mother, and his sister traveled to live with family in Palestine, which became the State of Israel in 1948. In the newly declared republic, military service was mandatory. Kahneman was called to service but accepted a deferred-enlistment plan to enroll at Hebrew University in 1952. He graduated two years later with a bachelor's degree in psychology and entered the Israeli Defense Forces as a second lieutenant. During his two years of service, Kahneman was involved in the psychological evaluation of officer candidates. The evaluation process sowed the seeds of Kahneman's later research on cognitive biases. "With a B.A. in the appropriate field," Kahneman wryly remarked in his Nobel Foundation autobiography, "I was the best-trained professional psychologist in the [Israeli] military."

### Research Collaboration and Themes

After leaving the Israeli army in 1956, Kahneman spent two more years at Hebrew University before traveling to the University of California, Berkeley, where he completed his PhD in 1961. He returned to Hebrew University in Jerusalem to become a lecturer and later a professor in the school's nascent psychology department. He gradually developed his own research agenda, which focused on the problems of judgment under uncertainty. Some of his early work involved the applications of psychology to public policy and to military practice. In one study recounted at length in _Thinking, Fast and Slow_, Kahneman examined the roles of praise and blame in the education of Israeli air force cadets. In _Thinking, Fast and Slow_, Kahneman also discusses some key results of his work during a sabbatical at the University of Michigan in 1965, where he studied the physiological markers of mental exertion.

In the late 1960s, Kahneman met psychologist Amos Tversky (1937–96), who became a lifelong friend and a frequent collaborator. The two psychologists coauthored several studies that laid the groundwork for heuristics-and-biases psychology. They published their landmark study "Judgment Under Uncertainty" in 1974. During the late 1970s, Kahneman and Tversky concentrated on synthesizing their observations into a theory regarding how individuals make economic decisions. Their framework, called _prospect theory_, accounts for some fundamental ways in which real people predictably depart from economic rationality. By the 1980s, Tversky had attained a star status on the basis of their joint work. In 1984 Tversky won a MacArthur Foundation fellowship, a financial award that supports creative intellectuals in their work.

From this point, Kahneman's and Tversky's work took different paths. According to a 2016 _New Yorker_ article by legal scholar and professor Cass Sunstein and Richard Thaler, it was geographic distance that ultimately led to a weakening of the Kahneman/Tversky collaboration. Tversky was a professor at Stanford University in California, and Kahneman was a professor at the University of British Columbia in Vancouver, and the two simply could not preserve their working relationship of earlier years.

### A Leading Thinker

Kahneman increasingly worked independently and sought out new collaborators in such fields as economics and legal theory. For example, Kahneman worked especially closely with American economist and researcher Richard Thaler (b. 1945), a 2017 Nobel laureate widely seen as one of the founders of behavioral economics. In 1993 Kahneman accepted a dual appointment at Princeton University as professor of psychology and professor of public affairs. Soon afterward, Amos Tversky was diagnosed with terminal cancer. Kahneman helped Tversky assemble an introduction and a collection of papers related to their work before Tversky died in 1996. In 2002 Kahneman shared the Nobel Prize in Economics with American economist Vernon L. Smith (b. 1927), a pioneer in the use of laboratory experiments in economic research. Kahneman has repeatedly claimed that, if not for his premature death, Tversky would also have shared the Nobel Prize. In _Thinking, Fast and Slow_ Kahneman repeatedly acknowledges Tversky's contributions to the work, specifically the ways in which Tversky challenged Kahneman's thinking in productive ways.

Kahneman formally retired from his Princeton professorship in 2007. He began working on a book project that was by then decades in the making. It was both a survey of a field of thought and in some ways a career retrospective. _Thinking, Fast and Slow_ was published in 2011 to critical and popular acclaim. Since then, Kahneman continued to receive national and international accolades for his work, including the Presidential Medal of Freedom in 2013 and honorary degrees from Yale in 2014, Cambridge in 2013, and numerous other institutions.
